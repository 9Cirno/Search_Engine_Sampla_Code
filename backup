import os
import re
import json
import sqlite3

import sys
import math
from HTMLParser import HTMLParser


#class MyHTMLParser(HTMLParser):
#    def handle_data(self, data):
#        return data
#parser = MyHTMLParser()
#for root, d, f in os.walk("../WEBPAGES_CLEAN/", topdown = False):

if os.path.exists('info.db'):
    conn = sqlite3.connect('info.db')
    conn.isolation_level = None
    print "Database connected successfully";
else:
    conn = sqlite3.connect('info.db')
    conn.isolation_level = None
    conn.execute('''
                CREATE TABLE Words_positions
                (word int PRIMARY KEY NOT NULL,
                position char(255) NOT NULL);
                ''')
    conn.execute('''
                CREATE TABLE Positions_urls
                (position  NOT NULL,
                url char(255) NOT NULL);
                ''')
    print "Table created successfully";

file_bookkeeping = open("bookkeeping.json", 'r')
content_bookkeeping = file_bookkeeping.read()
urls = json.loads(content_bookkeeping)
invertIndex = {}
outputLinks = list()
try:
    indexfile = open("Inverted_Index_Clean.json", 'a')
except IOError:
    indexfile = open("Inverted_Index_Clean.json", 'a')
process = 0
processcount = 0
count = 0

for root, d, f in os.walk("../WEBPAGES_CLEAN/", topdown = False):
    #print f
    for name in f:
        processcount += 1
        if(processcount/3700>=1):
            process+=10
            print str(process) + "% done"
            processcount = 0
        path_pattern = os.path.join(root,name).split("/")
        path = path_pattern[len(path_pattern)-2]+"/"+path_pattern[len(path_pattern)-1]
        if(path_pattern[len(path_pattern)-1].isdigit()):
            count += 1
            #print urls[path]
            fp = open(os.path.join(root,name), 'r')
            unique_words = set()
            lines = fp.readlines()
            for line in lines:
                line = re.sub(r'\<.*?\>', " ", line)
                for word in re.sub(r'\W', " ", line).split():
                    word = word.lower()
                    if word.isalpha() and len(word)<50:
                        unique_words.add(word)

            for word in unique_words:
                if invertIndex.__contains__(word):
                    invertIndex[word].append(path)
                else:
                    invertIndex[word]=[]
                    invertIndex[word].append(path)
print "done"

indexfile.write(json.dumps(invertIndex, ensure_ascii=False))
print "total unique words: " + str(len(invertIndex))
print "total document parsed: " + str(count)

print "query: Informatics"
print "Total Resule for word Informatics: " + str(len(invertIndex["informatics"]))
print "Example urls for Informatics:"
for i in range(0, 10):
    print urls[invertIndex["informatics"][i]]
print "-----------------"
print "query: Mondego"
print "Total Resule for word Mondego: " + str(len(invertIndex["mondego"]))
print "Example urls for Mondego:"
for i in range(0, 10):
    print urls[invertIndex["mondego"][i]]
print "-----------------"
print "query: Irvine"
print "Total Resule for word Irvine: " + str(len(invertIndex["irvine"]))
print "Example urls for Irvine:"
for i in range(0, 10):
    print urls[invertIndex["irvine"][i]]


# parser.feed(content)
#      for line in fp:
#          for word in line.split():
#             parser = MyHTMLParser()
#               parser.feed(word)

 #count_list = sorted(count_dict.items(), key=lambda kv: (-kv[1], kv[0]))
            #for i in count_list:
            #    print(i[0], " - ", i[1])
